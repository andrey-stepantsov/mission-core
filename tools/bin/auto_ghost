#!/usr/bin/env python3
import argparse
import sys
import os
import subprocess
import json
import re
from pathlib import Path

# --- Scanner Logic ---

class DependencyScanner:
    def __init__(self, include_paths, repo_root, max_depth=16):
        self.include_paths = [Path(p).resolve() for p in include_paths]
        self.repo_root = Path(repo_root).resolve()
        self.max_depth = max_depth
        self.visited = set()
        self.dependencies = set()
        self.cache = {} # path -> list of includes

    def find_header(self, header_name, current_file_path):
        """Resolves a header name to an absolute path."""
        # 1. Check relative to current file (for quoted includes)
        current_dir = Path(current_file_path).parent
        candidate = (current_dir / header_name).resolve()
        if candidate.exists():
            return candidate

        # 2. Check include paths (for <angled> and "quoted")
        for inc_path in self.include_paths:
            candidate = (inc_path / header_name).resolve()
            if candidate.exists():
                return candidate
        
        return None

    def scan_file(self, file_path, depth=0):
        """Recursively scans a file for #include directives."""
        abs_path = Path(file_path).resolve()
        
        if abs_path in self.visited:
            return
        
        self.visited.add(abs_path)
        
        if depth > self.max_depth:
            return

        # Add to dependencies if it's not the root file (which starts at depth 0)
        # However, for 'auto_ghost', we want the file itself AND its dependencies?
        # The prompt says: "implicit dependencies".
        # If we are ghosting a file to the container, we probably want the file + deps.
        # But if the file is the entry point, maybe we just want deps?
        # Let's keep existing logic: depth > 0
        if depth > 0:
            self.dependencies.add(str(abs_path))

        # Read file content
        try:
            content = abs_path.read_text(errors='ignore')
        except Exception as e:
            return

        # Regex for includes
        includes = re.findall(r'^\s*#\s*include\s+["<]([^">]+)[">]', content, re.MULTILINE)

        for header_name in includes:
            resolved_path = self.find_header(header_name, abs_path)
            if resolved_path:
                self.scan_file(resolved_path, depth + 1)

# --- Weave Logic ---

def get_weave_files(view_name, tools_root):
    """Invokes the weave tool to get files for a view."""
    weave_bin = Path(tools_root) / "bin" / "weave"
    if not weave_bin.exists():
        # Fallback to relative path if not found (e.g. running from repo root)
        # Try to find standard location
        weave_bin = Path(os.getcwd()) / ".mission" / "tools" / "bin" / "weave"
        if not weave_bin.exists():
             # Last resort: PATH?
             weave_bin = Path("weave")
    
    try:
        # We assume we are in the repo root for weave to find config, or weave handles it.
        # We pass --expand to get contexts as well?
        cmd = [str(weave_bin), "get", view_name, "--expand", "--json"]
        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=sys.stderr, text=True, check=True)
        # Weave outputs JSON now
        try:
            files = json.loads(result.stdout)
        except json.JSONDecodeError:
            # Fallback for old weave? Or just error.
            # Assuming we updated weave, so this should be fine. 
            print("Error decoding weave JSON", file=sys.stderr)
            return []
            
        return [str(Path(f).resolve()) for f in files if f]
    except Exception as e:
        print(f"Error invoking weave: {e}", file=sys.stderr)
        return []

# --- Main Script ---

def main():
    parser = argparse.ArgumentParser(description="Auto-Ghost: Dependency Detector")
    parser.add_argument("target_file", nargs="?", help="Specific file to scan")
    parser.add_argument("--view", help="Weave view to load if no target file", default="ghost")
    parser.add_argument("--format", choices=["json", "docker"], default="json", help="Output format")
    
    parser.add_argument("--mode", choices=["auto", "compiler", "scanner"], default="auto", help="Dependency scanning strategy")
    parser.add_argument("--full", action="store_true", help="Return full context including compile commands")
    parser.add_argument("--flags", help="Flags to filter compilation context")
    
    args = parser.parse_args()
    
    repo_root = os.getcwd() # Assume run from repo root
    script_dir = Path(os.path.dirname(os.path.realpath(__file__)))
    tools_root = script_dir.parent
    
    all_files = set()
    compile_context = None

    # Strategy 1: Target File
    if args.target_file:
        target_file = Path(args.target_file).resolve()
        
        # Locate c_context binary
        c_context_bin = script_dir / "c_context"
        
        if c_context_bin.exists():
            try:
                # Step 0: Get Context
                cmd = [str(c_context_bin), str(target_file)]
                if args.flags:
                     cmd.extend(["--flags", args.flags])
                
                result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=sys.stderr, text=True, check=True)
                context_data = json.loads(result.stdout)
                include_paths = context_data.get("includes", [])
                
                # Capture context for full output
                if args.full:
                    compile_context = {
                        "directory": context_data.get("directory"),
                        "arguments": context_data.get("arguments"),
                        "file": str(target_file),
                        "candidates": context_data.get("candidates", []),
                        "stats": context_data.get("stats", {})
                    }
                
                scan_success = False

                # Step 1: Compiler-based scan (gcc -M)
                if args.mode in ["auto", "compiler"]:
                    compile_args = context_data.get("arguments", [])
                    cwd = context_data.get("directory", repo_root)
                    
                    if compile_args:
                        try:
                            # Construct -M command
                            # Filter out -o, -c, -Werror
                            scan_cmd = []
                            skip_next = False
                            for arg in compile_args:
                                if skip_next:
                                    skip_next = False
                                    continue
                                if arg == "-o":
                                    skip_next = True
                                    continue
                                if arg in ["-c", "-Werror"]:
                                    continue
                                scan_cmd.append(arg)
                                
                            # Add -M
                            scan_cmd.append("-M")
                            
                            # Run it
                            res = subprocess.run(scan_cmd, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=True)
                            
                            # Parse Make Rule Output
                            # GCC outputs: "target: dep1 dep2 \n dep3 ..."
                            # We can't just split() because filenames might have spaces escaped as "\ "
                            output = res.stdout
                            
                            # 1. Normalize line continuations
                            output = output.replace("\\\n", "")
                            
                            # 2. Extract dependencies part (after ":")
                            if ":" in output:
                                _, deps_text = output.split(":", 1)
                            else:
                                deps_text = output # Fallback
                                
                            # 3. Parse with regex to handle escaped spaces
                            # Pattern: look for non-space sequences, allowing "\ "
                            # Or simpler: Split by space, but recombine if ended with backslash? 
                            # GCC escapes space as "\ ", so we can use regex
                            import re
                            # Matches sequence of (escaped char OR non-whitespace)
                            # But wait, we replaced \\\n with empty string (concatenation). 
                            # Escaped space in filename is likely "\ " in the output.
                            
                            # Regex strategy:
                            # 1. Split by whitespace *that is not preceded by a backslash*
                            # (?<!\\)\s+
                            raw_deps = re.split(r'(?<!\\)\s+', deps_text.strip())
                            
                            dependencies = []
                            for d in raw_deps:
                                if d:
                                    # invalid escape sequences might happen, but usually just "\ " -> " "
                                    clean = d.replace("\\ ", " ")
                                    dependencies.append(clean)
                            
                            for dep in dependencies:
                                dep_path = Path(cwd) / dep
                                if dep_path.exists():
                                    all_files.add(str(dep_path.resolve()))
                            
                            scan_success = True

                        except Exception as e:
                            if args.mode == "compiler":
                                print(f"Error: Compiler scan failed: {e}", file=sys.stderr)
                            else:
                                print(f"Warning: Compiler scan failed, failing back to scanner: {e}", file=sys.stderr)

                # Step 2: Regex scanning (Fallback or Explicit)
                if args.mode == "scanner" or (args.mode == "auto" and not scan_success):
                    scanner = DependencyScanner(include_paths, repo_root)
                    scanner.scan_file(target_file)
                    all_files.update(scanner.dependencies)


            except Exception as e:
                 print(f"Error scanning {target_file}: {e}", file=sys.stderr)
        else:
            # Maybe just return the file itself if not C? or nothing?
            # Existing behavior was error. Let's be lenient.
            pass

    # Strategy 2: Weave View
    elif args.view:
        # If no target file, use the view
        files = get_weave_files(args.view, tools_root)
        all_files.update(files)
        
    # Helpers for output
    sorted_files = sorted(list(all_files))
    
    if args.format == "json":
        if args.full:
            output = {
                "dependencies": sorted_files,
                "compile_context": compile_context
            }
            print(json.dumps(output, indent=2))
        else:
            print(json.dumps(sorted_files, indent=2))
        
    elif args.format == "docker":
        # -v /abs:/abs:ro
        flags = []
        for f in sorted_files:
            flags.append(f"-v {f}:{f}:ro")
        print(" ".join(flags))

if __name__ == "__main__":
    main()
